from mcmc import *
from itertools import product
from fn import F

random.seed(14361436)
np.random.seed(14361436)

# Union-Find of Hashables

def init_sets(iter):
    return {x: [x, 1] for x in iter}

def find(sets, x):
    p = sets[x][0]
    if p != x:
        p = sets[x][0] = find(sets, p)
    return p

def union(sets, x, y):
    r_x = find(sets, x)
    r_y = find(sets, y)
    if r_x == r_y:
        return r_x
    if sets[r_x][1] > sets[r_y][1]:
        sets[r_y][0] = r_x
        return r_x
    else:
        sets[r_x][0] = r_y
        if sets[r_x][1] == sets[r_y][1]:
            sets[r_y][1] += 1
        return r_y

# Gen a random BN w/ N rvs according to:
#   1. there is P probability that an edge exists to each rv from each of its
#       topologically smaller rv.
#   2. each rv is named 'n{i}' with I being its topological index (0-indexed).
#   3. each rv takes a value in {1, 2, ..., X} w/ X uniform in [A, ..., B].
#       The CPTs are generated according to Dirichlet([Uniform(0, 1.5 B) * X]).
#   4. if CONNECTED is TRUE and the dag generated by above 3 principles is not
#       connected, then random edges will be added to join the disjoint
#       components, and may affect the edges (rather than them solely dependent
#       on P).
#   5. VALUES controls what values the rvs take. Thus, len(VALUES) >= b. If
#       VALUES == INT, then natural number sequences will be used.
# Returns (the BayesNet object, a collection of all ccs).
def gen_random_bn(n, p = 0.5, a = 2, b = 5, connected = False, values = int):
    # params check
    assert n > 0 and type(n) == int, 'need n > 0 and n \in Z'
    assert 0 < a <= b and type(a) == type(b) == int, \
        'need 0 < a <= b and a, b \in Z'
    assert 0 <= p <= 1, 'need 0 <= p <= 1'
    assert values == int or len(values) >= b, 'not enough values for rvs to use'
    if values == int:
        values = range(b)
    values = np.array(values)
    # gen rvs according to first 3 principles
    rvs = np.arange(n)
    names = []
    rv_sizes = []
    parent_lists = []
    cc_lookup = init_sets(rvs)
    for i in rvs:
        names.append('n' + str(i))
        rv_sizes.append(np.random.randint(a, b + 1))
        parent_list = rvs[:i][np.random.rand(i) < p].tolist()
        # update cc_lookup
        for parent in parent_list:
            union(cc_lookup, i, parent)
        parent_lists.append(parent_list)
    # build ccs
    ccs = defaultdict(set)
    for i in rvs:
        ccs[find(cc_lookup, i)].add(i)
    # merge ccs if necessary
    if connected and len(ccs) > 1:
        while len(ccs) > 1:
            r_a, r_b = np.random.choice(ccs.keys(), 2, replace = False)
            n_a = np.random.choice(list(ccs[r_a]))
            n_b = np.random.choice(list(ccs[r_b]))
            if n_a > n_b:
                n_a, n_b = n_b, n_a
            parent_lists[n_b].append(n_a)
            r_all = union(cc_lookup, n_a, n_b)
            r_other = r_a if r_all == r_b else r_b
            ccs[r_all].update(ccs[r_other])
            ccs.pop(r_other)
    # build BN
    bn = BayesNet()
    for i, name, size, parent_list in \
        zip(rvs, names, rv_sizes, parent_lists):
        parent_names = map(names.__getitem__, parent_list)
        cpt = CPT(bn, name, parent_names, range(size))
        num_entries = reduce(mul, map(rv_sizes.__getitem__, parent_list), 1)
        for comb, diric_param in zip( \
            product(*map(F() >> rv_sizes.__getitem__ >> range >> values.__getitem__, parent_list)), \
            np.random.uniform(0, b * 1.5, (num_entries, size))
        ):
            parent_dict = {p: v for p, v in zip(parent_names, comb)}
            distn = np.random.dirichlet(diric_param)
            distn[-1] = 1.0 - sum(distn[:-1]) # force sum to be 1.0
            distn_dict = {v: prob for v, prob in zip(values, distn)}
            cpt.add_entry(parent_dict, distn_dict)
    bn.finalize()
    return bn, map(lambda cc: map(names.__getitem__, cc), ccs.values())

# Randomly generate an inference problem for the given BN.
# rv_type_distn defines a 3-categorical discrete distn w.p. of each r.v. being
# (evidence, hidden, query). For each evidence rv, its value is chosen uniformly
# random. However, it is enforced that there are at least one query variable.
# Returns a Problem object.
def gen_random_problem(bn, rv_type_distn = [0.35, 0.5, 0.15]):
    assert sum(rv_type_distn) == 1.0 and len(rv_type_distn) == 3, \
        'invalid rv type distribution'
    rv_types = (gen_random_problem.E, gen_random_problem.H, gen_random_problem.Q)
    rv_type_rv = \
        rv_discrete(name = 'rv type', values = (rv_types, tuple(rv_type_distn)))
    es = {}
    qs = []
    for rv in bn.rvs:
        t = rv_type_rv.rvs()
        if t == gen_random_problem.E:
            es[rv] = np.random.choice(bn[rv].values)
        elif t == gen_random_problem.Q:
            qs.append(rv)
    if len(qs) == 0:
        q = np.random.choice(bn.rvs.keys())
        if q in es:
            es.pop(q)
        qs.append(q)
    return Problem(bn, es, qs)

gen_random_problem.E = 0
gen_random_problem.H = 1
gen_random_problem.Q = 2

bn, ccs = gen_random_bn(12, a = 2, b = 2, values = (False, True), connected = True)

p = gen_random_problem(bn, rv_type_distn = [0.3, 0.45, 0.25])
print p

# s = Gibbs(p, verbose_int = 100, N = 200, T = 1000, record_start = 200)
# print s.infer()

# s = BinaryBNOneVarMaxESJD(p, verbose_int = 100, N = 200, T = 1000, record_start = 200)
# print s.infer()

s = AMCMC_NN(p, verbose_int = 100, N = 200, T = 1000, record_start = 200, \
    train_int =20, train_steps = 100, train_batch_size = 10, \
    train_alpha_fn = lambda t: 60.0 / (60 + t), \
    explore_ratio_fn = lambda t: 80.0 / (80 + t))
print s.infer(plot_lag = 10)

s = GibbsAll(p, verbose_int = 100, N = 200, T = 1000, record_start = 200)
print s.infer(plot_lag = 10)

# s = BinaryBNOneVarMaxESJDAll(p, verbose_int = 100, N = 200, T = 1000, record_start = 200)
# print s.infer()

# s = PerfectProposal(p, verbose_int = 100, N = 200, T = 1000, record_start = 200)
# print s.infer()

